{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Regression**"
      ],
      "metadata": {
        "id": "Z_btR_pa0PFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1 .**What is Simple Linear Regression?**\n",
        "\n",
        "**Answer**:- Simple Linear Regression is a method used to find the relationship between two variables — one independent variable (X) and one dependent variable (Y).\n",
        "\n",
        "2. **What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "**Answer**:- Key assumptions of Simple Linear Regression:\n",
        "\n",
        "* **Linearity**: The most important assumption of simple linear regression is that there is a linear relationship between the independent variable (X) and the dependent variable (Y). This means the change in Y is directly proportional to the change in X. If the data forms a straight-line pattern when plotted on a graph, this assumption is likely satisfied.\n",
        "\n",
        "* **Independence of Observations**: Each observation in the dataset should be independent of the others. This means the value of one data point should not influence or be related to the value of another. Violations of this assumption often occur in time-series data where one observation depends on previous ones.\n",
        "\n",
        "* **Homoscedasticity (Constant Variance)**: The residuals (errors between actual and predicted values) should have constant variance across all levels of the independent variable. In other words, the spread of the errors should not increase or decrease as the value of X changes. If the spread of errors widens or narrows, it indicates heteroscedasticity, which can weaken the model’s accuracy.\n",
        "\n",
        "* **Normality of Residuals**: The residuals or errors should be normally distributed. This is important when constructing confidence intervals and hypothesis tests. While the model can still function if this assumption is slightly violated, significant departures from normality can affect the reliability of statistical conclusions.\n",
        "\n",
        "3. **What does the coefficient m represent in the equation Y=mX+c?**\n",
        "\n",
        "**Answer**:- In the equation Y = mX + c, the coefficient m represents the slope of the line. It shows how much the dependent variable Y changes when the independent variable X increases by one unit. If m is positive, it means there is a positive relationship between X and Y—when X increases, Y also increases. On the other hand, if m is negative, it indicates a negative relationship—when X increases, Y decreases. The larger the absolute value of m, the steeper the slope of the line, which means a stronger effect of X on Y. So, the coefficient m tells us both the direction and the strength of the relationship between the two variables.\n",
        "\n",
        "4. **What does the intercept c represent in the equation Y=mX+c?**\n",
        "\n",
        "**Answer**:- In the equation Y = mX + c, the intercept c represents the point where the line crosses the Y-axis. This happens when the value of X is zero. In other words, c is the predicted value of Y when X = 0. It gives the starting value of Y before any change in X occurs.\n",
        "\n",
        "For example, if you're predicting someone's salary (Y) based on years of experience (X), the intercept c would represent the estimated salary for someone with zero years of experience. While sometimes the intercept may not have a practical meaning (especially if X = 0 is unrealistic), it is still an essential part of the equation to make accurate predictions.\n",
        "\n",
        "5. **How do we calculate the slope m in Simple Linear Regression?**\n",
        "\n",
        "**Answer**:- In the equation Y = mX + c, the intercept c represents the value of Y when X is zero. It is the point where the line crosses the Y-axis on a graph. This means that if you plug in X = 0 into the equation, the value of Y will be equal to c.\n",
        "\n",
        "For example, if you're predicting a person's weight based on their height, and the equation is Weight = 2 × Height + 30, then the intercept 30 is the predicted weight when the height is zero. While this might not always make real-world sense (like a height of zero), the intercept is still necessary for calculating accurate predictions from the model. It serves as the starting point of the regression line.\n",
        "\n",
        "6. **What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "\n",
        "**Answer**:- The purpose of the least squares method in Simple Linear Regression is to find the best-fitting straight line through the data points by minimizing the errors between the predicted values and the actual values. These errors are called residuals, which are the differences between the observed Y values and the predicted Y values from the regression line.\n",
        "\n",
        "The method works by squaring each of these residuals (to make them positive and give more weight to larger errors) and then summing them up. The line that results in the smallest total squared error is considered the best-fitting line. This is why it's called the \"least squares\" method—it finds the line that has the least total of squared residuals.\n",
        "\n",
        "7. **How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "\n",
        "**Answer**:- In Simple Linear Regression, the coefficient of determination, denoted as R², tells us how well the regression line explains the variation in the dependent variable (Y) based on the independent variable (X). It represents the proportion of the total variance in Y that can be predicted from X. For example, if R² is 0.8, it means that 80% of the changes in Y are explained by the changes in X, while the remaining 20% is unexplained—possibly due to noise or other variables not included in the model.\n",
        "\n",
        "An R² value of 1 means the regression line perfectly fits the data, while an R² value of 0 means the line does not explain any variation in Y. In practice, a higher R² indicates a better fit, but it does not always mean the model is perfect or appropriate—it just means more of Y’s variation is captured by X. So, R² helps us assess how useful the linear model is for making predictions.\n",
        "\n",
        "8. **What is Multiple Linear Regression?**\n",
        "\n",
        "**Answer**:- Multiple Linear Regression is an extension of Simple Linear Regression used when we want to predict a dependent variable (Y) using two or more independent variables (X₁, X₂, X₃, ...) instead of just one. It helps us understand how several factors together influence the outcome.\n",
        "\n",
        "The general equation for Multiple Linear Regression is: Y = β₀ + β₁X₁ + β₂X₂ + β₃X₃ + ... + βnXn\n",
        "\n",
        "In this equation, Y is the predicted value, β₀ is the intercept, and β₁, β₂, β₃, etc., are the coefficients (slopes) that represent how much Y changes with a one-unit change in each corresponding X variable, keeping the others constant. This method is useful when outcomes depend on more than one factor, such as predicting a house price based on size, location, and number of rooms all at once.\n",
        "\n",
        "9. **What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "**Answer**:- The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable. In Simple Linear Regression, there is only one independent variable (X) used to explain or predict the dependent variabl (Y). The model looks like Y = β₀ + β₁X, and it fits a straight line to the data based on the relationship between just two variables.\n",
        "\n",
        "In contrast, Multiple Linear Regression involves two or more independent variables (X₁, X₂, X₃, ...) to predict the same dependent variable. Its equation looks like Y = β₀ + β₁X₁ + β₂X₂ + ... + βnXn, and it fits a plane (or hyperplane) in higher dimensions rather than just a line. This allows the model to consider more factors at once, giving a more complex and potentially more accurate prediction when multiple inputs affect the outcome.\n",
        "\n",
        "10. **What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "**Answer**:- Multiple Linear Regression has several key assumptions that must be met to ensure the model produces reliable and meaningful results.\n",
        "\n",
        "First, the relationship between the dependent variable and each of the independent variables should be linear. This means that the effect of each predictor on the outcome is constant, and the combined effects form a straight-line relationship in higher-dimensional space. Second, the residuals, which are the differences between the observed and predicted values, should be normally distributed. This assumption is important for conducting statistical tests on the regression coefficients.\n",
        "\n",
        "Another assumption is that the residuals must have constant variance across all levels of the independent variables, a condition known as homoscedasticity. Additionally, there should be no or very little multicollinearity, which means that the independent variables should not be highly correlated with one another. Finally, the observations should be independent of each other, meaning the data points should not be related or influence one another. Meeting these assumptions helps ensure that the model is stable, interpretable, and valid for making predictions.\n",
        "\n",
        "11. **What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "\n",
        "**Answer**:- Heteroscedasticity occurs when the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variables. In simpler terms, it means that the spread of the errors changes depending on the value of the predictors—sometimes the errors are small, and other times they are much larger.\n",
        "\n",
        "This uneven spread can negatively affect a Multiple Linear Regression model. While the estimated coefficients themselves may still be unbiased, the standard errors of those estimates become unreliable, which can lead to incorrect confidence intervals and p-values. As a result, you might wrongly conclude that a variable is statistically significant when it is not, or miss a truly significant effect. In essence, heteroscedasticity weakens the trust you can place in the statistical inferences drawn from the model, even if the predictions may still look reasonable.\n",
        "\n",
        "12. **How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "**Answer**:- When a Multiple Linear Regression model suffers from high multicollinearity, it means that two or more independent variables are highly correlated with each other. This makes it difficult for the model to accurately estimate the effect of each variable on the dependent variable, as their influences are overlapping.\n",
        "\n",
        "To improve the model, one approach is to remove one of the correlated variables if they are conveying similar information. Another option is to combine the correlated variables into a single feature using techniques like creating an average or using Principal Component Analysis (PCA). You can also try regularization methods, such as Ridge or Lasso Regression, which help reduce the impact of multicollinearity by shrinking the coefficients. By applying these techniques, the model becomes more stable and the interpretation of each variable’s effect becomes clearer.\n",
        "\n",
        "13. **What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "\n",
        "**Answer**:- Categorical variables need to be transformed into a numerical format before they can be used in regression models, as regression algorithms cannot directly interpret text or labels. One of the most common techniques is one-hot encoding, where each category is turned into a separate binary (0 or 1) column. For example, if a variable has three categories like \"Red\", \"Blue\", and \"Green\", it becomes three columns: one for each color, with a 1 indicating presence and 0 indicating absence.\n",
        "\n",
        "Another popular method is label encoding, where each category is assigned a unique number, such as 1 for \"Red\", 2 for \"Blue\", and 3 for \"Green\". However, this method is usually avoided in linear regression because the numbers can imply an order or ranking that may not exist. For ordinal categorical variables, where the categories have a natural order (like \"Low\", \"Medium\", \"High\"), ordinal encoding can be appropriate, where the labels are replaced by numbers that reflect their order. Choosing the right transformation depends on the nature of the data and the model being used.\n",
        "\n",
        "14. **What is the role of interaction terms in Multiple Linear Regression?**\n",
        "\n",
        "**Answer**:- In Multiple Linear Regression, interaction terms are used to capture situations where the effect of one independent variable on the dependent variable depends on the value of another independent variable. They allow the model to go beyond simple additive relationships and account for combined effects between variables.\n",
        "\n",
        "For example, suppose you're predicting a person's salary based on education level and years of experience. If the impact of education on salary is stronger for people with more experience, then there's an interaction between education and experience. By including an interaction term (usually by multiplying the two variables), the model can better capture this relationship. Without interaction terms, the model assumes that the effect of each variable is independent and constant, which may not reflect the real-world complexity of the data.\n",
        "\n",
        "15. **How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "\n",
        "**Answer**:- In Simple Linear Regression, the intercept represents the predicted value of the dependent variable (Y) when the independent variable (X) is zero. It gives a straightforward starting point: if X = 0, then Y equals the intercept. This interpretation is usually easy to understand, though it may not always be meaningful in real-world contexts if X = 0 is unrealistic.\n",
        "\n",
        "In Multiple Linear Regression, the interpretation of the intercept becomes more complex. It represents the predicted value of Y when all independent variables (X₁, X₂, X₃, etc.) are equal to zero. This situation might be unlikely or even impossible depending on the variables. For example, if one of the inputs is \"years of education\" or \"income,\" a value of zero may not make sense. Therefore, while the intercept is still mathematically necessary for the model, its practical interpretation in multiple regression is often less meaningful or even ignored in favor of focusing on the other coefficients.\n",
        "\n",
        "16. **What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "\n",
        "**Answer**:- The slope in regression analysis represents the rate of change in the dependent variable (Y) for a one-unit increase in the independent variable (X), while keeping other variables constant. It tells us the strength and direction of the relationship between the predictor and the outcome. A positive slope means that as X increases, Y also increases, while a negative slope indicates that Y decreases as X increases.\n",
        "\n",
        "The slope is crucial for making predictions because it directly determines how changes in the input variable influence the predicted value of the output. If the slope is large in magnitude, small changes in X will lead to big changes in Y. On the other hand, a slope close to zero suggests that X has little to no impact on Y. Therefore, the slope helps us understand both the influence of the variable and how to use the regression model to make accurate forecasts.\n",
        "\n",
        "17. **How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "**Answer**:- The intercept in a regression model provides the starting point or baseline value of the dependent variable (Y) when all the independent variables (X values) are equal to zero. It gives context by anchoring the regression line or plane, allowing us to understand how the dependent variable behaves in the absence of any input from the predictors.\n",
        "\n",
        "While the intercept may not always have a practical or realistic meaning—especially if a value of zero for the predictors is not possible—it still plays an important role in the model’s structure. It ensures that the line fits the data correctly and allows us to calculate accurate predicted values. In cases where X = 0 is meaningful, the intercept can give direct insight into the natural or default level of Y before any influencing factors come into play.\n",
        "\n",
        "18. **What are the limitations of using R² as a sole measure of model performance?**\n",
        "\n",
        "**Answer**:- Using R² as the sole measure of model performance has several important limitations. While R² tells us how much of the variation in the dependent variable is explained by the model, it does not indicate whether the model is actually a good fit in terms of accuracy or reliability. A high R² can sometimes be misleading, especially if the model is overfitting the data by capturing noise instead of real patterns.\n",
        "\n",
        "Moreover, R² does not account for the number of predictors used. Adding more independent variables will never decrease R²—it can only stay the same or increase, even if the new variables have no real explanatory power. This can give a false impression of improvement. Also, R² does not tell us whether the regression coefficients are statistically significant or whether the model makes accurate predictions on new, unseen data. Therefore, it should always be used alongside other metrics, such as adjusted R², mean squared error, or validation scores, to get a complete picture of model performance.\n",
        "\n",
        "19. **How would you interpret a large standard error for a regression coefficient?**\n",
        "\n",
        "**Answer**:- A large standard error for a regression coefficient indicates that the estimate of that coefficient is not very precise. It suggests that the value of the coefficient could vary a lot if the model were built on a different sample of data. In other words, there's a lot of uncertainty around that coefficient’s true value.\n",
        "\n",
        "This uncertainty affects how confident we can be that the variable has a real impact on the dependent variable. If the standard error is large compared to the size of the coefficient itself, it may result in a high p-value, which means the variable might not be statistically significant. As a result, we might question whether that variable truly belongs in the model or whether its observed effect is just due to random variation in the data.\n",
        "\n",
        "20. **How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "**Answer**:- Heteroscedasticity can be identified in residual plots by looking at how the residuals (errors) are spread across the range of predicted values. In a well-behaved model with constant variance (homoscedasticity), the residuals will appear randomly scattered around zero with roughly equal spread. However, if heteroscedasticity is present, the residual plot will show a clear pattern, such as a funnel shape (widening or narrowing spread) or clusters that grow larger or smaller across the x-axis.\n",
        "\n",
        "It is important to address heteroscedasticity because it can lead to biased or inefficient estimates of standard errors, which affects the reliability of confidence intervals and hypothesis tests. Even though the regression coefficients themselves may still be correct, their interpretation becomes risky, and the conclusions drawn from the model might be misleading. Fixing heteroscedasticity helps ensure that statistical inferences and predictions made by the model remain valid and trustworthy.\n",
        "\n",
        "21. **What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "\n",
        "**Answer**:- If a Multiple Linear Regression model has a high R² but a low adjusted R², it means that adding more independent variables may not be truly improving the model’s performance. While R² always increases or stays the same when new variables are added, adjusted R² takes into account both the number of predictors and the size of the dataset. It penalizes the model for including variables that do not provide meaningful contribution.\n",
        "\n",
        "A large gap between R² and adjusted R² suggests that some of the predictors in the model may be irrelevant or redundant, and are possibly just adding noise rather than explaining real variation in the dependent variable. This is a warning sign that the model might be overfitting the data—fitting the training data too closely without improving its ability to generalize to new, unseen data.\n",
        "\n",
        "22. **Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "**Answer**:- Scaling variables in Multiple Linear Regression is important because it ensures that all independent variables contribute fairly to the model, especially when they are measured on different scales. If one variable has values ranging in thousands while another ranges between 0 and 1, the larger-scaled variable may dominate the calculations, even if it’s not more important.\n",
        "\n",
        "Although standard linear regression coefficients are still correct without scaling, certain tasks—like interpreting coefficients, improving numerical stability, or applying regularization techniques (such as Ridge or Lasso regression)—benefit greatly from scaling. It helps the model converge faster and makes the interpretation of coefficients more meaningful when comparing the relative influence of predictors.\n",
        "\n",
        "23. **What is polynomial regression?**\n",
        "\n",
        "**Answer**:- Polynomial regression is a type of regression analysis used when the relationship between the independent variable (X) and the dependent variable (Y) is non-linear but can still be modeled using a polynomial function. Instead of fitting a straight line as in linear regression, polynomial regression fits a curved line by adding higher-degree terms of the independent variable.\n",
        "\n",
        "The general form of a polynomial regression equation is: Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βnXⁿ\n",
        "\n",
        "Here, the powers of X (like X², X³, etc.) allow the model to capture bends and curves in the data. Despite the curved shape of the line, polynomial regression is still considered a linear model because it is linear in terms of the coefficients. It is especially useful when data shows patterns that a straight line cannot capture, but care must be taken not to use too high a degree, as it can lead to overfitting.\n",
        "\n",
        "24. **How does polynomial regression differ from linear regression?**\n",
        "\n",
        "**Answer**:- Polynomial regression differs from linear regression in the type of relationship it models between the independent and dependent variables. In linear regression, the model assumes a straight-line relationship, meaning the change in Y is proportional to the change in X. The equation involves only the first power of X, such as Y = β₀ + β₁X.\n",
        "\n",
        "In contrast, polynomial regression allows for a curved relationship by including higher powers of the independent variable, like X², X³, and so on. The equation looks like Y = β₀ + β₁X + β₂X² + β₃X³ + ..., which enables the model to capture more complex patterns in the data. While both models are linear in terms of the coefficients, polynomial regression fits data that linear regression cannot, especially when the pattern between X and Y is not a straight line.\n",
        "\n",
        "25. **When is polynomial regression used?**\n",
        "\n",
        "**Answer**:- Polynomial regression is used when the relationship between the independent variable and the dependent variable is non-linear, but can still be captured by a smooth, curved line. It is helpful in situations where a straight-line model (linear regression) fails to fit the data accurately and leaves large residuals or clear patterns in the error.\n",
        "\n",
        "For example, polynomial regression is commonly used when modeling things like growth curves, pricing patterns, or physical phenomena where the rate of change itself changes over time. It is especially useful in datasets where the trend bends upward or downward, such as U-shaped or inverted U-shaped patterns. However, it’s important to choose the degree of the polynomial carefully, because using too high a degree can lead to overfitting—fitting the noise rather than the underlying trend.\n",
        "\n",
        "26. **What is the general equation for polynomial regression?**\n",
        "\n",
        "**Answer**:- The general equation for polynomial regression is an extension of the simple linear regression formula and is used to model non-linear relationships between the independent and dependent variables. It is written as Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ + ε. In this equation, Y represents the predicted value, X is the independent variable, and β₀ is the intercept. The terms β₁, β₂, up to βₙ are the coefficients for each corresponding power of X, and ε is the error term that accounts for the difference between the observed and predicted values.\n",
        "\n",
        "This form allows the model to fit a curve rather than a straight line by including powers of X, such as X², X³, and so on, depending on the degree of the polynomial. The higher the degree, the more flexible the curve becomes, allowing it to capture complex patterns in the data. However, choosing the right degree is important, as very high degrees can lead to overfitting.\n",
        "\n",
        "27. **Can polynomial regression be applied to multiple variables?**\n",
        "\n",
        "**Answer**:- Yes, polynomial regression can be applied to multiple variables, and this is known as multivariate polynomial regression. In this case, the model includes not only powers of individual variables but also their interactions and combinations. For example, if you have two variables X₁ and X₂, the polynomial regression model might include terms like X₁², X₂², X₁·X₂, and even higher-order combinations depending on the chosen degree.\n",
        "\n",
        "This allows the model to capture more complex relationships between the input variables and the target variable. However, as the number of variables and the polynomial degree increase, the number of terms in the model grows rapidly. This can lead to overfitting and increased computational cost, so it's important to use techniques like regularization or feature selection when working with multivariate polynomial regression.\n",
        "\n",
        "28. **What are the limitations of polynomial regression?**\n",
        "\n",
        "**Answer**:- Polynomial regression, while powerful for modeling non-linear relationships, has several important limitations that should be considered. One major issue is overfitting, especially when using high-degree polynomials. The model may fit the training data too closely, capturing noise instead of the underlying pattern, which leads to poor performance on new, unseen data.\n",
        "\n",
        "Another limitation is that polynomial regression can become unstable outside the range of the training data, causing the predictions to swing wildly, especially at the edges. It also tends to be sensitive to outliers, as a single unusual point can distort the shape of the curve significantly. In addition, as the degree of the polynomial and the number of input variables increase, the model becomes more complex and computationally expensive, with many terms to calculate and interpret. This complexity can make the model harder to explain and less generalizable in practical scenarios.\n",
        "\n",
        "29. **What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "**Answer**:- When selecting the degree of a polynomial for regression, it’s important to evaluate how well the model fits the data without overfitting. One commonly used method is cross-validation, where the data is split into training and validation sets multiple times to test how the model performs on unseen data. This helps identify the degree that balances accuracy and generalization.\n",
        "\n",
        "Another approach is to look at evaluation metrics such as Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) on both the training and validation sets. A model that performs well on training data but poorly on validation data likely has too high a degree and is overfitting. Additionally, adjusted R² can be used instead of regular R² because it accounts for the number of terms in the model, preventing the false impression of improvement when unnecessary complexity is added. By comparing these metrics across different polynomial degrees, you can choose the one that offers the best trade-off between bias and variance.\n",
        "\n",
        "30. **Why is visualization important in polynomial regression?**\n",
        "\n",
        "**Answer**:- Visualization is important in polynomial regression because it helps you understand how well the model fits the data, especially when the relationship is non-linear. By plotting the data points along with the fitted polynomial curve, you can visually assess whether the curve captures the underlying trend or if it is underfitting or overfitting the data.\n",
        "\n",
        "It also allows you to detect issues that might not be obvious from numerical metrics alone—such as erratic behavior at the edges of the plot or sudden swings in the curve caused by a high-degree polynomial. Additionally, visualization can reveal outliers or patterns in the residuals that may affect the model’s performance. In short, seeing the model’s behavior makes it easier to interpret, trust, and improve your regression analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "FR2sWkI50VNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. **How is polynomial regression implemented in Python?**\n",
        "\n",
        "**Answer**:- Polynomial regression in Python is commonly implemented using the PolynomialFeatures class from scikit-learn, combined with linear regression. The idea is to first transform your input data to include polynomial terms and then fit a linear regression model to that transformed data."
      ],
      "metadata": {
        "id": "9rfV9iB13xWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here’s a basic example of how it’s done:\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 5, 10, 17, 26])\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlvZCynx4C2i",
        "outputId": "a88acffb-d8b9-4fe5-98ea-328b39917843"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [ 0.00000000e+00 -8.77076189e-15  1.00000000e+00]\n",
            "Intercept: 1.000000000000007\n"
          ]
        }
      ]
    }
  ]
}